{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "9f396ff1-9ad4-41a3-b042-5ebcb38ed94d",
      "cell_type": "markdown",
      "source": "# Introduction\n\nThis notebook documents the extract, transform, load process for the QuantumTalent AI market analytics project.\nThe objective is to prepare a clean and standardized dataset for HR analytics by implementing a medallion\narchitecture composed of Bronze, silver and gold layers\n\nWhat will this notebook include? \n- Database and schema initialization\n- Bronze layer: raw data ingestion\n- Silver layer: data cleaning and standardization\n- The Gold layer: dimensional modeling, star schema, KPIs and measures ",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      }
    },
    {
      "id": "c23cca7a-2dad-454e-a19e-5fb880e7831e",
      "cell_type": "markdown",
      "source": "# ETL Architecture Overview\n\nThis project follows a **medallion architecture** approach:\n\n- Bronze layer: stores raw, unprocessed data as received from the source CSV file.\n- Silver layer: contains cleaned, standardized, and business-ready data.\n- Gold layer: analytical models and KPIs\n\ngoal of this separation: To improve data quality, traceability, and scalability of the analytics pipeline.\n",
      "metadata": {}
    },
    {
      "id": "4688b656-dce5-4c7b-8c0f-5981106f8f54",
      "cell_type": "code",
      "source": "# database intialization \n\nuse master; \ncreate database ai_jobs; \n\nuse ai_jobs;\n\n#creation of bronze layer ( for row data )# \ncreate schema bronze; \ngo \n#creation of silver layer ( for cleaned data ) # \ncreate schema silver; \ngo \n# creation of gold layer ( for analysis ready data)# \ncreate schema gold;\ngo ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0b21ab78-2651-473e-bf80-89fc770eef6d",
      "cell_type": "markdown",
      "source": "## Bronze Layer – Raw Data Ingestion\n\nThe Bronze layer stores raw AI job market data exactly as received from the source.\nNo transformations, validations, or business rules are applied at this stage.\n\nThe purpose of this layer is to:\n- Preserve original data\n- Enable traceability\n- Act as a reliable ingestion source for downstream transformations\n",
      "metadata": {}
    },
    {
      "id": "b3d8d44c-1a43-4a6c-ad9a-6fe8e2a780d5",
      "cell_type": "code",
      "source": "use ai_jobs; \n\n#drop table if it existts\nIF OBJECT_ID('bronze.ai_jobs', 'U') IS NOT NULL\nBEGIN\n    DROP TABLE bronze.ai_jobs;\nEND\nGO \n\nCREATE TABLE bronze.ai_jobs (\n    job_id                  VARCHAR(50),\n    job_title               VARCHAR(255),\n    salary_usd              VARCHAR(50),\n    salary_currency         VARCHAR(10),\n    experience_level        VARCHAR(50),\n    employment_type         VARCHAR(50),\n    company_location        VARCHAR(100),\n    company_size            VARCHAR(50),\n    employee_residence      VARCHAR(100),\n    remote_ratio            VARCHAR(50),\n    required_skills         VARCHAR(MAX),\n    education_required      VARCHAR(100),\n    years_experience        VARCHAR(50),\n    industry                VARCHAR(100),\n    posting_date            VARCHAR(50),\n    application_deadline    VARCHAR(50),\n    job_description_length  VARCHAR(50),\n    benefits_score          VARCHAR(50),\n    company_name            VARCHAR(255)\n);\nGO \n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "35dae110-9aec-4d4d-8017-e12b546d212a",
      "cell_type": "code",
      "source": "# Load CSV into Bronze Layer \n\nTRUNCATE TABLE bronze.ai_jobs;\nBULK INSERT bronze.ai_jobs\nFROM 'C:\\Users\\user\\Desktop\\BIIIII\\ai_job_dataset (2).csv'\nWITH (\n    FIRSTROW = 2,\n    FIELDTERMINATOR = ',',\n    ROWTERMINATOR = '\\n',\n    TABLOCK\n);\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f76f07aa-9317-4cd5-bab2-2ee69d88845f",
      "cell_type": "markdown",
      "source": "## Bronze Layer Validation\n\nAfter ingestion, basic validation checks are performed to ensure that:\n- Data was loaded successfully\n- Row counts are consistent\n- Columns are populated correctly\n",
      "metadata": {}
    },
    {
      "id": "eefbafbc-2c50-4841-9d22-0e7407346910",
      "cell_type": "code",
      "source": "select * from bronze.ai_jobs; \n\nSELECT COUNT(*) AS total_rows\nFROM bronze.ai_jobs;\n\nSELECT TOP 10 *\nFROM bronze.ai_jobs;\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3f467d7d-3cfb-4754-94c6-9ab18aeff5ac",
      "cell_type": "markdown",
      "source": "# Silver Layer : transformation of data\n\nThe **Silver layer** is the intermediate stage of the ETL pipeline. Its main purpose is to **clean, standardize, and structure raw data** from the Bronze layer, making it ready for analytics in the gold layer.\n\n\n> **goal** Data is clean, validated, and structured, making it suitable for  building dimensions and fact tables in the Gold layer.\n",
      "metadata": {}
    },
    {
      "id": "c4bb60f9-501f-4adb-84af-6aea25f6620c",
      "cell_type": "markdown",
      "source": "- ## Example of transformations carried\n\n1. **Create Cleaned Tables**\n   - Droped existing tables to avoid conflicts.\n   - Defined tables with proper column types.\n2. **Transform Raw Data**\n   - Used `TRY_CAST` to safely convert numeric or date columns.\n   - Applied `CASE` statements to categorize values.\n   - Trim text fields to remove extra spaces.\n   - Categorize numerical scores (e.g., benefits_score → Poor, Fair, Good, Excellent)\n3. **Normalize skills column**\n   - Split `skills` into a separate table for one-to-many relationships.\n4. **Validate**\n   - Check for nulls, duplicates, and malformed data before moving to the Gold layer.\n",
      "metadata": {}
    },
    {
      "id": "73833511-f590-467d-be4f-78428305cc50",
      "cell_type": "code",
      "source": "#DDL for silver table#\n\n#Drop the silver table if it exists to avoind conflics\nIF OBJECT_ID('silver.ai_jobs_clean', 'U') IS NOT NULL\nBEGIN\n    DROP TABLE silver.ai_jobs_clean;\nEND\nGO \n\n#creation of the silver table\nCREATE TABLE silver.ai_jobs_clean (\n    job_id VARCHAR(50) PRIMARY KEY,\n    job_title VARCHAR(255) NOT NULL,\n    salary_usd DECIMAL(15,2),\n    experience_level VARCHAR(50),\n    employment_type VARCHAR(50),\n    company_location VARCHAR(100),\n    company_size VARCHAR(50),\n    employee_residence VARCHAR(100),\n    remote_ratio VARCHAR(50),\n    required_skills VARCHAR(MAX), \n    education_required VARCHAR(50),\n    years_experience INT,\n    experience_range VARCHAR(20),  #new col\n    industry VARCHAR(100),\n    posting_date VARCHAR(20),\n    benefits_score VARCHAR(20),\n    company_name VARCHAR(255),\n    posting_year INT  #new col\n);\n\n#populate the silver table with modifications#\nTRUNCATE TABLE silver.ai_jobs_clean;\n\nINSERT INTO silver.ai_jobs_clean (\n    job_id, job_title, salary_usd, experience_level, employment_type,\n    company_location, company_size, employee_residence, remote_ratio,\n    required_skills, education_required, years_experience,experience_range, industry,\n    posting_date,\n    benefits_score, company_name, posting_year\n)\nSELECT\n    job_id,\n    TRIM(job_title),\n    TRY_CAST(\n    CASE \n        WHEN TRY_CAST(salary_usd AS DECIMAL(15,2)) <= 0 THEN NULL #check for outliers\n        ELSE salary_usd\n    END AS DECIMAL(15,2)\n) AS salary_usd,\n    CASE experience_level\n        WHEN 'EN' THEN 'Entry'\n        WHEN 'MI' THEN 'Mid'\n        WHEN 'SE' THEN 'Senior'\n        WHEN 'EX' THEN 'Executive'\n        ELSE experience_level\n    END AS experience_level,\n    CASE employment_type\n        when 'FL' THEN 'Freelance'\n        WHEN 'PT' Then 'Part Time'\n        WHEN 'CT' THEN 'Contract'\n        WHEN 'FT' THEN ' Full Time'\n        ELSE employment_type \n      END AS employment_type,\n    company_location,\n    CASE company_size\n        WHEN 'S' THEN 'Small'\n        WHEN 'M' THEN 'Medium'\n        WHEN 'L' THEN 'Large'\n        ELSE company_size\n    END AS company_size,\n    employee_residence,\n    CASE remote_ratio\n        WHEN '100' THEN 'Remote'\n        WHEN '50' THEN 'Hybrid'\n        WHEN '0' THEN 'On-site'\n        ELSE remote_ratio\n    END AS work_mode,\n    required_skills,\n    education_required,\n    TRY_CAST(years_experience AS INT),\n    CASE \n        WHEN years_experience  BETWEEN 0 AND 2 THEN '0–2'\n        WHEN years_experience  BETWEEN 3 AND 5 THEN '3–5'\n        WHEN years_experience  BETWEEN 6 AND 10 THEN '6–10'\n        WHEN years_experience  > 10 THEN '10+'\n        ELSE 'Unknown'\n    END AS experience_range,\n    industry,\n    FORMAT(TRY_CAST(LTRIM(RTRIM(posting_date)) AS DATE), 'dd-MM-yyyy') AS posting_date, #formatting the date\n    CASE \n        WHEN TRY_CAST(benefits_score AS DECIMAL(3,1)) IS NULL THEN 'Unknown'\n        WHEN TRY_CAST(benefits_score AS DECIMAL(3,1)) < 6.0 THEN 'Poor'\n        WHEN TRY_CAST(benefits_score AS DECIMAL(3,1)) < 7.5 THEN 'Fair'\n        WHEN TRY_CAST(benefits_score AS DECIMAL(3,1)) < 8.5 THEN 'Good'\n        WHEN TRY_CAST(benefits_score AS DECIMAL(3,1)) < 9.5 THEN 'Excellent'\n        ELSE 'Outstanding'\n    END AS benefits_category,\n    company_name,\n    YEAR(TRY_CAST(posting_date AS DATE)) AS posting_year # add the posting year col\nFROM bronze.ai_jobs; \n\n#create table for job skills: # \nDROP TABLE IF EXISTS silver.job_skills;\nCREATE TABLE silver.job_skills (\n    job_id VARCHAR(50),\n    skill_name VARCHAR(100)\n);\n\nINSERT INTO silver.job_skills (job_id, skill_name)\nSELECT \n    job_id,\n    TRIM(value) AS skill_name\nFROM silver.ai_jobs_clean\nCROSS APPLY STRING_SPLIT(required_skills, ',') \n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "61dda329-c064-4fc0-9c3c-2e60ee3a223d",
      "cell_type": "markdown",
      "source": "# Silver Layer Validation",
      "metadata": {}
    },
    {
      "id": "6adb9579-2e73-4119-b1b1-e1ee8b5840d0",
      "cell_type": "code",
      "source": "#check for duplicates in the primary key(jod_id)\nSELECT job_id, COUNT(*) AS count\nFROM silver.ai_jobs_clean\nGROUP BY job_id\nHAVING COUNT(*) > 1; \n\n# text standarization (example for job_title variations) \nSELECT DISTINCT job_title\nFROM silver.ai_jobs_clean\nORDER BY job_title;\n\n#make sure all the data from bronze layer is imported\nSELECT \n    (SELECT COUNT(*) FROM bronze.ai_jobs_raw) AS bronze_count,\n    (SELECT COUNT(*) FROM silver.ai_jobs_clean) AS silver_count;\n\n#check for reasonable numeric ranges\nSELECT MIN(salary_usd) AS min_salary, MAX(salary_usd) AS max_salary\nFROM silver.ai_jobs_clean;\n\n#check the categorical values addded\nSELECT DISTINCT benefits_score_category\nFROM silver.ai_jobs_clean;\n\n#Sample Data Review \nSELECT TOP 10 * FROM silver.ai_jobs_clean ORDER BY NEWID();\n\n# review all dataset \nselect * from silver.ai_jobs_clean;",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}